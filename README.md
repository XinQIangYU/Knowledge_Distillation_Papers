# Knowledge_Distillation_Papers
This are knowledge distillation papers.
## CVPR2023

1. DaFKD : Domain-aware Federated Knowledge Distillation 
2. Generalization Matters: Loss Minima Flattening via Parameter Hybridization for Efficient Online Knowledge Distillation
3. DisWOT: Student Architecture Search for Distillation WithOut Training
4. Generic-to-Specific Distillation of Masked Autoencoders
5. Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks
6. Learning to Retain while Acquiring: Combating Distribution-Shift in Adversarial Data-Free Knowledge Distillation
